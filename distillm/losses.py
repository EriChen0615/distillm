import torch
import torch.nn.functional as F

def forward_kl(logits, teacher_logits, no_model_batch, vocab_size):
    #NOTE JC: Compute forward KL divergence with vocabulary alignment.
    # Align both student and teacher logits to tokenizer's vocabulary size
    logits = align_logits_to_vocab(logits, vocab_size)
    teacher_logits = align_logits_to_vocab(teacher_logits, vocab_size)

    teacher_probs = F.softmax(teacher_logits, dim=-1, dtype=torch.float32)
    inf_mask = torch.isinf(logits)
    student_logprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)
    prod_probs = torch.masked_fill(teacher_probs * student_logprobs, inf_mask, 0)
    x = torch.sum(prod_probs, dim=-1).view(-1)
    mask = (no_model_batch["label"] != -100).int()
    distil_loss = -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)
    return distil_loss

def reverse_kl(logits, teacher_logits, no_model_batch, vocab_size):
    #NOTE JC: Compute reverse KL divergence with vocabulary alignment.
    # Align both student and teacher logits to tokenizer's vocabulary size
    logits = align_logits_to_vocab(logits, vocab_size)
    teacher_logits = align_logits_to_vocab(teacher_logits, vocab_size)

    student_probs = F.softmax(logits, dim=-1, dtype=torch.float32)
    student_logprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)
    teacher_logprobs = F.log_softmax(teacher_logits, dim=-1, dtype=torch.float32)
    inf_mask = torch.isinf(teacher_logits) | torch.isinf(logits)
    prod_probs = torch.masked_fill(student_probs * teacher_logprobs, inf_mask, 0)
    prod_probs -= torch.masked_fill(student_probs * student_logprobs, inf_mask, 0)
    x = torch.sum(prod_probs, dim=-1).view(-1)
    mask = (no_model_batch["label"] != -100).int()
    distil_loss = -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)
    return distil_loss

def symmetric_kl(logits, teacher_logits, no_model_batch, vocab_size, lam=0.9):
    #NOTE JC: Compute symmetric KL divergence with vocabulary alignment.
    # Align both student and teacher logits to tokenizer's vocabulary size
    logits = align_logits_to_vocab(logits, vocab_size)
    teacher_logits = align_logits_to_vocab(teacher_logits, vocab_size)

    for_kl = forward_kl(logits, teacher_logits, no_model_batch)
    rev_kl = reverse_kl(logits, teacher_logits, no_model_batch)
    distil_loss = (1-lam) * for_kl + lam * rev_kl
    return distil_loss
    
def js_distance(logits, teacher_logits, no_model_batch, vocab_size, lam=0.9):
    #NOTE JC: Compute JS distance with vocabulary alignment.
    # Align both student and teacher logits to tokenizer's vocabulary size
    logits = align_logits_to_vocab(logits, vocab_size)
    teacher_logits = align_logits_to_vocab(teacher_logits, vocab_size)

    teacher_probs = F.softmax(teacher_logits, dim=-1, dtype=torch.float32)
    student_probs = F.softmax(logits, dim=-1, dtype=torch.float32)
    mixed_probs = (1-lam) * teacher_probs + lam * student_probs

    teacher_logprobs = F.log_softmax(teacher_logits, dim=-1, dtype=torch.float32)
    student_logprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)
    mixed_logprobs = torch.log(mixed_probs)

    mask = (no_model_batch["label"] != -100).int()
    inf_mask = torch.isinf(logits) | torch.isinf(teacher_logits)

    prod_probs = torch.masked_fill(student_probs * mixed_logprobs, inf_mask, 0)
    prod_probs -= torch.masked_fill(student_probs * student_logprobs, inf_mask, 0)
    x = torch.sum(prod_probs, dim=-1).view(-1)
    distil_loss = lam * -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)

    prod_probs = torch.masked_fill(teacher_probs * mixed_logprobs, inf_mask, 0)
    prod_probs -= torch.masked_fill(teacher_probs * teacher_logprobs, inf_mask, 0)
    x = torch.sum(prod_probs, dim=-1).view(-1)
    distil_loss += (1-lam) * -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)
    return distil_loss
    
def tv_distance(logits, teacher_logits, no_model_batch, vocab_size):
    #NOTE JC: Compute TV distance with vocabulary alignment.
    # Align both student and teacher logits to tokenizer's vocabulary size
    logits = align_logits_to_vocab(logits, vocab_size)
    teacher_logits = align_logits_to_vocab(teacher_logits, vocab_size)

    teacher_probs = F.softmax(teacher_logits, dim=-1, dtype=torch.float32)
    student_probs = F.softmax(logits, dim=-1, dtype=torch.float32)
    
    mask = (no_model_batch["label"] != -100).int()
    inf_mask = torch.isinf(logits) | torch.isinf(teacher_logits)
    prod_probs = 0.5 * torch.masked_fill(torch.abs(teacher_probs - student_probs), inf_mask, 0)
    x = torch.sum(prod_probs, dim=-1).view(-1)
    distil_loss = torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)
    return distil_loss

def skewed_forward_kl(logits, teacher_logits, no_model_batch, vocab_size, lam=0.1):
    #NOTE JC: Compute skewed forward KL divergence with vocabulary alignment.
    # Align both student and teacher logits to tokenizer's vocabulary size
    logits = align_logits_to_vocab(logits, vocab_size)
    teacher_logits = align_logits_to_vocab(teacher_logits, vocab_size)

    teacher_probs = F.softmax(teacher_logits, dim=-1, dtype=torch.float32)
    student_probs = F.softmax(logits, dim=-1, dtype=torch.float32)
    mixed_probs = lam * teacher_probs + (1-lam) * student_probs
    mixed_logprobs = torch.log(mixed_probs)
    
    mask = (no_model_batch["label"] != -100).int()
    inf_mask = torch.isinf(logits) | torch.isinf(teacher_logits)

    prod_probs = torch.masked_fill(teacher_probs * mixed_logprobs, inf_mask, 0)
    x = torch.sum(prod_probs, dim=-1).view(-1)
    distil_loss = -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)
    return distil_loss

def skewed_reverse_kl(logits, teacher_logits, no_model_batch, vocab_size, lam=0.1):
    #NOTE JC: Compute skewed forward KL divergence with vocabulary alignment.
    # Align both student and teacher logits to tokenizer's vocabulary size
    student_logits = align_logits_to_vocab(student_logits, vocab_size)
    teacher_logits = align_logits_to_vocab(teacher_logits, vocab_size)

    teacher_probs = F.softmax(teacher_logits, dim=-1, dtype=torch.float32)
    student_probs = F.softmax(logits, dim=-1, dtype=torch.float32)
    mixed_probs = (1-lam) * teacher_probs + lam * student_probs
    
    student_logprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)
    mixed_logprobs = torch.log(mixed_probs)

    mask = (no_model_batch["label"] != -100).int()
    inf_mask = torch.isinf(logits) | torch.isinf(teacher_logits)

    prod_probs = torch.masked_fill(student_probs * mixed_logprobs, inf_mask, 0)
    prod_probs -= torch.masked_fill(student_probs * student_logprobs, inf_mask, 0)
    x = torch.sum(prod_probs, dim=-1).view(-1)
    distil_loss = -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)
    return distil_loss

#NOTE JC
def align_logits_to_vocab(logits, vocab_size):
    """
    Align logits to tokenizer's vocabulary size by truncating extra dimensions.
    Args:
        logits: Model output logits
        vocab_size: Target vocabulary size (tokenizer's vocabulary size)
    Returns:
        Truncated logits matching the tokenizer's vocabulary size
    """
    current_vocab_size = logits.size(-1)
    if current_vocab_size > vocab_size:
        return logits[..., :vocab_size]
    return logits